{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miczkejedrzej/MNLP-project-1/blob/main/01_Data_collection_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import moduls"
      ],
      "metadata": {
        "id": "mcnApD7NlnCY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6XveCXMZI-4I",
        "outputId": "7690a522-016e-44c0-a3da-13eccb6fc853",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/491.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m481.3/491.4 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/193.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/143.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/194.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mMounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install wikidata --quiet\n",
        "!pip install datasets --quiet\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "from wikidata.client import Client\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/France_Poland_multilanguage_shared_folder/[MNLP 2025 HW1] train set [PUBLIC] - train_cleaned.tsv',sep='\\t')\n",
        "df_dev = pd.read_csv('/content/drive/MyDrive/France_Poland_multilanguage_shared_folder/validation_df.csv', index_col=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to extract the text"
      ],
      "metadata": {
        "id": "9V2kAGDBlqdY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions to extract the Wikipedia text for a given item"
      ],
      "metadata": {
        "id": "CCT154Pk5QQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_entity_id(url):\n",
        "    return url.strip().split(\"/\")[-1]\n",
        "\n",
        "def get_wikipedia_extract(wikidata_url):\n",
        "    \"\"\"return the Wikipedia text for a given wikidata url\"\"\"\n",
        "\n",
        "    entity_id = extract_entity_id(wikidata_url)\n",
        "\n",
        "    client = Client()\n",
        "    try:\n",
        "        item = client.get(entity_id, load=True)\n",
        "        # Get english wikipedia sitelink\n",
        "        sitelinks = item.data.get(\"sitelinks\", {})\n",
        "        enwiki = sitelinks.get(\"enwiki\")\n",
        "\n",
        "        if enwiki:\n",
        "            title = enwiki[\"title\"]\n",
        "\n",
        "            #Wikipedia API to get the text\n",
        "            api_url = \"https://en.wikipedia.org/w/api.php\"\n",
        "            headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "            params = {\n",
        "                \"action\": \"query\",\n",
        "                \"prop\": \"extracts\",\n",
        "                \"explaintext\": True,\n",
        "                \"titles\": title,\n",
        "                \"format\": \"json\",\n",
        "                \"redirects\": 1\n",
        "            }\n",
        "\n",
        "            res = requests.get(api_url, params=params, headers=headers).json()\n",
        "            page = next(iter(res[\"query\"][\"pages\"].values()))\n",
        "            return page.get(\"extract\", \"No extract found.\")[:1000] #1000 caracters\n",
        "        else:\n",
        "            return \"No English Wikipedia article found for this entity.\"\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        if e.response.status_code == 404:\n",
        "            print(f\"Entity not found for URL: {wikidata_url}\")\n",
        "            return \"Entity not found in Wikidata.\"\n",
        "        else:\n",
        "            raise e  # Re-raise other HTTP errors\n",
        "    except Exception as e:\n",
        "        # Catch any other exception that may have occurred during Wikidata entity retrieval\n",
        "        print(f\"An unexpected error occurred for URL: {wikidata_url}. Error: {e}\")\n",
        "        return \"Error retrieving Wikidata entity.\""
      ],
      "metadata": {
        "id": "GEypOQfDRE6o"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Application to the train dataset"
      ],
      "metadata": {
        "id": "GH8cQgfzlw8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Application to the dataset\n",
        "tqdm.pandas()\n",
        "df_train['text'] = df_train['item'].progress_apply(get_wikipedia_extract)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXV268TD3P6z",
        "outputId": "0b434698-26ad-4ad6-9ed2-3b342f2b0e9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 33%|███▎      | 2060/6251 [12:41<17:40,  3.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An unexpected error occurred for URL: http://www.wikidata.org/entity/Q7551241. Error: HTTP Error 404: Not Found\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 61%|██████    | 3812/6251 [23:41<32:53,  1.24it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking item without text\n",
        "df_train[(df_train['text'] == 'Error retrieving Wikidata entity.') |\n",
        "          (df_train['text'] == \"No English Wikipedia article found for this entity.\")]"
      ],
      "metadata": {
        "id": "ljRJK2Sy7qFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Application to the dev set"
      ],
      "metadata": {
        "id": "co4dCLSE45Fq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Application to the dev set\n",
        "df_dev['text'] = df_dev['item'].progress_apply(get_wikipedia_extract)"
      ],
      "metadata": {
        "id": "U7EapmqBxedo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking item without text\n",
        "df_dev[(df_dev['text'] == 'Error retrieving Wikidata entity.') |\n",
        "          (df_dev['text'] == \"No English Wikipedia article found for this entity.\")]"
      ],
      "metadata": {
        "id": "9gYLuTHn6x99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exportation"
      ],
      "metadata": {
        "id": "10PBMZLbl9FZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.to_json('train_with_text.json', orient='records', lines=True)\n",
        "df_dev.to_json('dev_with_text.json', orient='records', lines=True)\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "files.download('train_with_text.json')\n",
        "files.download('dev_with_text.json')"
      ],
      "metadata": {
        "id": "stmuHRQgX43v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}