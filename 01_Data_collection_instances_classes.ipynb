{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miczkejedrzej/MNLP-project-1/blob/main/01_Data_collection_instances_classes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhyM5XzTyOP3",
        "outputId": "f1cb39dd-c6d3-47e7-93ab-b4dac359049e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Import of the data\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "!pip install wikidata --quiet\n",
        "\n",
        "import pandas as pd\n",
        "from wikidata.client import Client\n",
        "import requests\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "from functools import lru_cache\n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/France_Poland_multilanguage_shared_folder/[MNLP 2025 HW1] train set [PUBLIC] - train_cleaned.tsv',sep='\\t')\n",
        "df_dev = pd.read_csv('/content/drive/MyDrive/France_Poland_multilanguage_shared_folder/validation_df.csv', index_col=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmWiWRMsnsn4"
      },
      "source": [
        "# Subclasses (P279)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH-sW4bbBoaR"
      },
      "source": [
        "The P279 property refers to the subclass, example : volcano is subclass of moutain.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfzETnR-ztpu"
      },
      "source": [
        "## Subclass depth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iuyxC7TzqoP"
      },
      "source": [
        " We are looking for the shortest path from the given item to a 'root' item that has not any subclass. We then define the depth of each item in the graph of subclasses as the number of nodes in the shortest path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SW4cxTElK6Ic"
      },
      "outputs": [],
      "source": [
        "WIKIDATA_SPARQL_URL = \"https://query.wikidata.org/sparql\"\n",
        "HEADERS = {\n",
        "    \"Accept\": \"application/sparql-results+json\",\n",
        "    \"User-Agent\": \"ExplorateurWikidata/0.1 (truc@truc.com)\"\n",
        "}\n",
        "\n",
        "def extract_entity_id(url):\n",
        "    return url.strip().split(\"/\")[-1]\n",
        "\n",
        "#cache avoir requesting the same item several times during the graph search\n",
        "@lru_cache(maxsize=None)\n",
        "\n",
        "def get_superclasses(qid):\n",
        "    \"\"\"get the superclasses (P279)\"\"\"\n",
        "\n",
        "    query = f\"\"\"\n",
        "    SELECT ?superclass WHERE {{\n",
        "      wd:{qid} wdt:P279 ?superclass.\n",
        "    }}\n",
        "    \"\"\"\n",
        "    response = requests.get(WIKIDATA_SPARQL_URL, params={\"query\": query}, headers=HEADERS)\n",
        "    results = response.json()[\"results\"][\"bindings\"]\n",
        "    return tuple(r[\"superclass\"][\"value\"].split(\"/\")[-1] for r in results)\n",
        "\n",
        "def get_subclass_depth_bfs(url, max_depth=20):\n",
        "    \"\"\"get the depth by finding the shortest path with BFS\"\"\"\n",
        "    qid = extract_entity_id(url)\n",
        "    visited = set()\n",
        "    queue = deque([(qid, 1)])\n",
        "\n",
        "    while queue:\n",
        "        current_qid, depth = queue.popleft()\n",
        "        if current_qid in visited:\n",
        "            continue\n",
        "        visited.add(current_qid)\n",
        "\n",
        "        parents = get_superclasses(current_qid)\n",
        "        if not parents:\n",
        "            return depth\n",
        "        if depth >= max_depth:\n",
        "            return None\n",
        "        for parent_qid in parents:\n",
        "            queue.append((parent_qid, depth + 1))\n",
        "\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TA4IihufOXEc",
        "outputId": "cd4c7b04-9782-427d-b623-3d9c71d02ce1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6251/6251 [17:11<00:00,  6.06it/s]\n"
          ]
        }
      ],
      "source": [
        "# Application to the datasets\n",
        "tqdm.pandas()\n",
        "df_train[\"subclass_depth\"]= df_train[\"item\"].progress_apply(get_subclass_depth_bfs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_dev[\"subclass_depth\"]= df_dev[\"item\"].progress_apply(get_subclass_depth_bfs)"
      ],
      "metadata": {
        "id": "z3Ylwk85kPa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P55KXbc4mZR"
      },
      "source": [
        "## Number of subclasses downward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRJKmd1Rz0rN"
      },
      "source": [
        "here were a looking for the number of item that are subclass of the given item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFYBCWP5xtIH"
      },
      "outputs": [],
      "source": [
        "def get_direct_subclasses(url):\n",
        "    \"\"\"return the number of subclasses \"\"\"\n",
        "    qid = extract_entity_id(url)\n",
        "    query = f\"\"\"\n",
        "    SELECT ?subclass WHERE {{\n",
        "      ?subclass wdt:P279 wd:{qid} .\n",
        "    }}\n",
        "    \"\"\"\n",
        "    response = requests.get(WIKIDATA_SPARQL_URL, params={\"query\": query}, headers=HEADERS)\n",
        "    results = response.json()[\"results\"][\"bindings\"]\n",
        "\n",
        "    return len(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vrzw2mBTx9Q"
      },
      "outputs": [],
      "source": [
        "# Application to the datasets\n",
        "df_train[\"subclasses\"] = df_train[\"item\"].apply(get_direct_subclasses)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_dev[\"subclasses\"] = df_dev[\"item\"].apply(get_direct_subclasses)"
      ],
      "metadata": {
        "id": "6DamAc0ukSZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtwuIGHw5KWl"
      },
      "source": [
        "# Instances of (P31) downward\n",
        "\n",
        "Here we explore the concept of the instances of the entity. Mainly we check how many entities on the wikipedia are the instances of the given entity, later we group the outcomes by the labels and plot them to see whether there is any meaningfull correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqtO-UBP4XaY"
      },
      "outputs": [],
      "source": [
        "def get_instances_of(url):\n",
        "  qid = extract_entity_id(url)\n",
        "  \"\"\"return  number of items that are 'instances of' the given entity\"\"\"\n",
        "  query = f\"\"\"\n",
        "  SELECT ?entity ?label WHERE {{\n",
        "    ?entity wdt:P31 wd:{qid}.\n",
        "    ?entity rdfs:label ?label .\n",
        "    FILTER(LANG(?label) = \"en\")\n",
        "  }}\n",
        "  \"\"\"\n",
        "  response = requests.get(WIKIDATA_SPARQL_URL, params={\"query\": query}, headers=HEADERS)\n",
        "  results = response.json()[\"results\"][\"bindings\"]\n",
        "\n",
        "  return len(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2FzgGaE6O6c"
      },
      "outputs": [],
      "source": [
        "# Application to the datasets\n",
        "df_train[\"instances_of\"] = df_train[\"item\"].apply(get_instances_of)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_dev[\"instances_of\"] = df_dev[\"item\"].apply(get_instances_of)"
      ],
      "metadata": {
        "id": "NN6xM1eAkUAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQSiQnYUoSuR"
      },
      "source": [
        "# Instances of (P31) upward\n",
        "Again instances of but now in reversed way how many other entites the given item is an instance of"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHsc38YYocC2"
      },
      "outputs": [],
      "source": [
        "def get_instances_of_up(url):\n",
        "  qid = extract_entity_id(url)\n",
        "  \"\"\"return  number of  'instances of' the given entity\"\"\"\n",
        "  query = f\"\"\"\n",
        "  SELECT ?class WHERE {{\n",
        "      wd:{qid} wdt:P31 ?class .\n",
        "    }}\n",
        "  \"\"\"\n",
        "  response = requests.get(WIKIDATA_SPARQL_URL, params={\"query\": query}, headers=HEADERS)\n",
        "  results = response.json()[\"results\"][\"bindings\"]\n",
        "\n",
        "  return len(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13ih_TDtoiZl"
      },
      "outputs": [],
      "source": [
        "# Application to the datasets\n",
        "df_train[\"instances_of_up\"] = df_train[\"item\"].apply(get_instances_of_up)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_dev[\"instances_of_up\"] = df_dev[\"item\"].apply(get_instances_of_up)"
      ],
      "metadata": {
        "id": "2gq5D3N4kV2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exportation"
      ],
      "metadata": {
        "id": "YJIbBuxgj8Of"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7FOsp_uBApy"
      },
      "outputs": [],
      "source": [
        "df_train.to_json('trainset_subclass_instances.json', orient='records', lines=True)\n",
        "df_dev.to_json('devset_subclass_instances.json', orient='records', lines=True)\n",
        "\n",
        "from google.colab import files\n",
        "files.download('trainset_subclass_instances.json')\n",
        "files.download('devset_subclass_instances.json')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}